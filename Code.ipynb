{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work is done in the context of Machine Learning Course at TBS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Dorra\\Pictures\\ML\\mushroom_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nb of entries in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns and types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provided is already cleaned, but we will check nonetheless:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().iloc[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation between Class and the rest of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.corr()['class'][:-1] # -1 to remove the last row which is class\n",
    "df_corr.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from Data Exploration:\n",
    "- Dataset has 54035 rows and 9 columns.\n",
    "- Data type of all columns is numerical (float or integer).\n",
    "- All values are non-null. Therefore no missing values.\n",
    "- Correlation between class and feature columns in df is low, absolute value ranging between 5% and 18.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has already undergone z-score normalization, so we will skip that step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the z-scores for each column\n",
    "z_scores = pd.DataFrame(stats.zscore(df), columns=df.columns)\n",
    "\n",
    "# Generate descriptive statistics for the z-scores\n",
    "z_scores.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows where any of the z-scores exceed the threshold\n",
    "outliers = z_scores[(np.abs(z_scores) > 3).any(axis=1)]\n",
    "\n",
    "# Drop the identified rows containing outliers\n",
    "df_no_outliers = df.drop(outliers.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop rows containing outliers (with Z-score > 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of rows of original dataframe, of new one and how many rows were removed\n",
    "new_num_r = df_no_outliers.shape[0]\n",
    "old_num_r = df.shape[0]\n",
    "removed = old_num_r - new_num_r\n",
    "\n",
    "print(\"New dataframe has {} rows. {} rows were removed.\".format(new_num_r, removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set_theme()\n",
    "\n",
    "# Create subplots with 3 columns and 3 rows\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over each column in the DataFrame\n",
    "for i, column in enumerate(df_no_outliers.columns):\n",
    "    # Create a histogram plot for the current column with hue\n",
    "    sns.histplot(data=df_no_outliers, x=column, hue='class', kde=True, bins=20, ax=axes[i])\n",
    "    \n",
    "    # Set title for the plot\n",
    "    axes[i].set_title(f'Distribution of {column}')\n",
    "    \n",
    "# Adjust layout to prevent overlap of titles\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create a count plot to visualize the distribution of 'cap-shape' with hue by 'class'\n",
    "sns.countplot(hue='class', x='cap-shape', data=df_no_outliers)\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Cap Shape Counts by Class')\n",
    "plt.legend(title='Class', labels=['0 = edible', '1 = poisonous'])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "# Create a count plot to visualize the distribution of 'gill-attachment' with hue by 'class'\n",
    "sns.countplot(hue='class', x='gill-attachment', data=df_no_outliers)\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Gill Attachment Counts by Class')\n",
    "plt.legend(title='Class', labels=['0 = edible', '1 = poisonous'])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between classes are evident in the histograms and count plots. On average poisonous mushrooms have smaller cap diameters and taller, slimmer stems compared to edible ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a variable X equal to the numerical features and a variable y equal to the \"class\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_no_outliers.loc[:, df_no_outliers.columns != \"class\"]\n",
    "y = df_no_outliers['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Test Split: We must ensure all models use the same test and train sets so that we guarantee a fair compairison later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=101, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spot Checking: I will compare basic versions of different models and compare according to accuracy as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Spot Checking with Multiple Classification Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=3),\n",
    "    \"Support Vector Machine\": SVC(kernel='linear'),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)  # Train the model\n",
    "    preds = model.predict(X_test)  # Make predictions\n",
    "    acc = accuracy_score(y_test, preds)  # Compute accuracy\n",
    "    results[name] = acc\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "results_df = pd.DataFrame(list(results.items()), columns=[\"Model\", \"Accuracy\"]).sort_values(by=\"Accuracy\", ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"Spot Checking Results:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional Machine Learning Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the hyperparameters and fit models using L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Define a custom grid for Cs to ensure a wide range of values are tested\n",
    "custom_cs = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# L1 regularized logistic regression with cross-validation\n",
    "lr_l1 = LogisticRegressionCV(Cs=custom_cs, cv=5, penalty='l1', solver='liblinear', verbose=0)\n",
    "\n",
    "# Fit the model on the training data\n",
    "lr_l1.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best C value\n",
    "best_C = lr_l1.C_[0]\n",
    "print(f\"Best C value: {best_C}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Extract the coefficients of the best model\n",
    "best_coefficients = lr_l1.coef_\n",
    "print(f\"Coefficients of the best model: {best_coefficients}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Extract the mean cross-validated scores for each fold and each parameter\n",
    "cv_scores = lr_l1.scores_[1]  # Assuming binary classification with target classes 0 and 1\n",
    "print(f\"Cross-validated scores for each parameter: {cv_scores}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Optionally, you can also find the mean cross-validated score for the best parameter\n",
    "best_score = cv_scores.mean(axis=0)[custom_cs.index(best_C)]\n",
    "print(f\"Mean cross-validated score for the best C value: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularized logistic regression\n",
    "lr_l2 = LogisticRegressionCV(Cs=custom_cs, cv=5, penalty='l2', solver='liblinear')\n",
    "lr_l2.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best C value\n",
    "best_C = lr_l2.C_[0]\n",
    "print(f\"Best C value: {best_C}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Extract the coefficients of the best model\n",
    "best_coefficients = lr_l2.coef_\n",
    "print(f\"Coefficients of the best model: {best_coefficients}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Extract the mean cross-validated scores for each fold and each parameter\n",
    "cv_scores = lr_l2.scores_[1]  # Assuming binary classification with target classes 0 and 1\n",
    "print(f\"Cross-validated scores for each parameter: {cv_scores}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Optionally, you can also find the mean cross-validated score for the best parameter\n",
    "best_score = cv_scores.mean(axis=0)[custom_cs.index(best_C)]\n",
    "print(f\"Mean cross-validated score for the best C value: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are quite low and very similar. Let's proceed with the L2 regularized model. \n",
    "Now, we will predict the class for L2 regularized model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_preds = lr_l2.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "print(classification_report(y_test, l2_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(y_test, l2_preds, normalize='true')\n",
    "\n",
    "sns.set_theme(style=\"white\", context=\"talk\")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cf)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, 73% of poisonous mushrooms are labeled correctly. But we have a big confusion in edible mushrooms, almost half of them (45%) are labeled as poisonous. We will need different model. Let's try another simple model KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- K-Nearest Neighbors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with k=1, and later will choose better K value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_preds = knn.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, knn_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,knn_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a K Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = []\n",
    "\n",
    "for i in range(1,20):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,20), error_rate, color='blue', linestyle='dashed', marker='o',\n",
    "        markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that at K=3 the error rate is the lowest and it's around 0.013. Let's retrain the KNN model with K=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "knn_preds = knn.predict(X_test)\n",
    "\n",
    "print('K Nearest Neighbors')\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test,knn_preds))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,knn_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(y_test, knn_preds, normalize='true')\n",
    "\n",
    "sns.set_context('talk')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cf,display_labels=knn.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
